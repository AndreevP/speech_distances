{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "textile-shoulder",
   "metadata": {},
   "source": [
    "# Test VCC2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "yellow-reply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-28 21:35:08 cloud:56] Found existing object /home/user01/.cache/torch/NeMo/NeMo_1.0.0rc1/speakerverification_speakernet/2202d9ccf2dccac4c87faf6b5507eb9d/speakerverification_speakernet.nemo.\n",
      "[NeMo I 2021-05-28 21:35:08 cloud:62] Re-using file from: /home/user01/.cache/torch/NeMo/NeMo_1.0.0rc1/speakerverification_speakernet/2202d9ccf2dccac4c87faf6b5507eb9d/speakerverification_speakernet.nemo\n",
      "[NeMo I 2021-05-28 21:35:08 common:654] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-05-28 21:35:08 modelPT:133] Please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /ws/manifests/raid/combined/train_manifest.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    time_length: 8\n",
      "    augmentor:\n",
      "      noise:\n",
      "        manifest_path: /ws/manifests/raid/musan/musan_music_noise_manifest_dur8.json\n",
      "        prob: 0.2\n",
      "        min_snr_db: 5\n",
      "        max_snr_db: 15\n",
      "    num_workers: 4\n",
      "    \n",
      "[NeMo W 2021-05-28 21:35:08 modelPT:140] Please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /ws/manifests/raid/voxceleb/small_manifest.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    time_length: 8\n",
      "    num_workers: 1\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-05-28 21:35:08 features:240] PADDING: 16\n",
      "[NeMo I 2021-05-28 21:35:08 features:256] STFT using torch\n",
      "[NeMo I 2021-05-28 21:35:08 label_models:88] Training with Angular Softmax Loss\n",
      "[NeMo I 2021-05-28 21:35:08 modelPT:376] Model EncDecSpeakerLabelModel was successfully restored from /home/user01/.cache/torch/NeMo/NeMo_1.0.0rc1/speakerverification_speakernet/2202d9ccf2dccac4c87faf6b5507eb9d/speakerverification_speakernet.nemo.\n",
      "[NeMo I 2021-05-28 21:35:08 features:240] PADDING: 16\n",
      "[NeMo I 2021-05-28 21:35:08 features:256] STFT using torch\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import dataset\n",
    "import torch    \n",
    "from collections import OrderedDict\n",
    "\n",
    "def extract_prefix(prefix, weights):\n",
    "    result = OrderedDict()\n",
    "    for key in weights:\n",
    "        if key.find(prefix) == 0:\n",
    "            result[key[len(prefix):]] = weights[key]\n",
    "    return result     \n",
    "\n",
    "\n",
    "ds = dataset.VCC2018DatasetNoPreporocess(list_path='../MOSNet/data/mos_list.txt', data_path='../MOSNet/data/wav/')\n",
    "model = models.NemoMOS().cuda()\n",
    "model.load_state_dict(extract_prefix('model.', torch.load('/home/user01/Downloads/speakernet-verif-tune.ckpt')['state_dict']), strict=False)\n",
    "_=model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "communist-boating",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:33<00:00, 119.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import dataset\n",
    "\n",
    "\n",
    "pred_mos = []\n",
    "gt_mos   = []\n",
    "for i in tqdm.trange(ds.getlen('test')):\n",
    "    item = ds.getitem('test', i)\n",
    "    x, l = dataset.collate_fn_lenth([item])[0]\n",
    "    with torch.no_grad():\n",
    "        res = model((x.cuda(), l.cuda())).mean()\n",
    "    \n",
    "    gt_mos.append(item[1].item())\n",
    "    pred_mos.append(res.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "muslim-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "mse = ((np.array(pred_mos)-np.array(gt_mos))**2).mean()\n",
    "lcc = np.corrcoef(np.array(gt_mos), np.array(pred_mos))[0,1]\n",
    "srcc = ss.spearmanr(np.array(gt_mos), np.array(pred_mos))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "induced-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.samples['test'].columns = ['audio', 'true_mos']\n",
    "df = ds.samples['test'].copy()\n",
    "df['predict_mos'] = np.array(pred_mos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "representative-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sys_df = pd.read_csv('../MOSNet/data/vcc2018_system.csv')\n",
    "df['system_ID'] = df['audio'].str.split('_').str[-1].str.split('.').str[0] + '_' + df['audio'].str.split('_').str[0]\n",
    "result_mean = df[['system_ID', 'predict_mos']].groupby(['system_ID']).mean()\n",
    "mer_df = pd.merge(result_mean, sys_df, on='system_ID')                                                                                                                 \n",
    "\n",
    "sys_true = mer_df['mean']\n",
    "sys_predicted = mer_df['predict_mos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "resident-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_mse = ((sys_true-sys_predicted)**2).mean()\n",
    "sys_lcc = np.corrcoef(sys_true, sys_predicted)[0,1]\n",
    "sys_srcc = ss.spearmanr(sys_true, sys_predicted)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "determined-treatment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Utterance] MSE = 0.4912 LCC = 0.7162 SRCC = 0.6971\n",
      "[System   ] MSE = 0.0811 LCC = 0.9723 SRCC = 0.9442\n"
     ]
    }
   ],
   "source": [
    "print('[Utterance] MSE = {:.4f} LCC = {:.4f} SRCC = {:.4f}'.format(mse, lcc, srcc))\n",
    "print('[System   ] MSE = {:.4f} LCC = {:.4f} SRCC = {:.4f}'.format(sys_mse, sys_lcc, sys_srcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-leader",
   "metadata": {},
   "source": [
    "# Test VCC2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "intended-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "eh1 = pd.read_csv(StringIO('\\n'.join([s.strip().replace(':MOS', '') for s in open('mos_EH1.txt', 'r').readlines()])), sep='\\t')\n",
    "eh2 = pd.read_csv(StringIO('\\n'.join([s.strip().replace(':MOS', '') for s in open('mos_EH2.txt', 'r').readlines()])), sep='\\t')\n",
    "\n",
    "mos_vcc2016 = pd.DataFrame(pd.concat([eh1, eh2]).mean())\n",
    "mos_vcc2016.columns = ['system_mos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "sticky-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "vcc2016 = glob.glob('../vcc2016_submissions/*/*.wav')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'audio': vcc2016,\n",
    "    'system': [s.split('/')[-2] for s in vcc2016]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "jewish-radar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26028/26028 [03:30<00:00, 123.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import scipy.signal\n",
    "\n",
    "def load(path):\n",
    "    signal,sr = librosa.load(path, sr=16000)\n",
    "    #spec = np.abs(librosa.stft(signal, n_fft=512, hop_length=256, win_length=512, window=scipy.signal.hamming).T).astype(np.float32) # [time, 257]\n",
    "\n",
    "    return torch.as_tensor(signal).unsqueeze(0), torch.tensor([len(signal)], dtype=torch.long)#.unsqueeze(1)\n",
    "\n",
    "    \n",
    "pred_mos = []\n",
    "\n",
    "for i in tqdm.trange(df.shape[0]):\n",
    "    signal, l = load(df.iloc[i]['audio'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        res = model((signal.cuda(), l.cuda())).mean()\n",
    "    \n",
    "    pred_mos.append(res.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "governing-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_mos'] = np.array(pred_mos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "regular-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_mos = np.array(mos_vcc2016.sort_index()['system_mos'])\n",
    "pred_mos = np.array(df.groupby('system').mean()['pred_mos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "directed-mention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.4023 LCC = 0.9126 SRCC = 0.8917\n"
     ]
    }
   ],
   "source": [
    "def score(gt, pred):\n",
    "    mse = ((gt-pred)**2).mean()\n",
    "    lcc = np.corrcoef(gt, pred)[0,1]\n",
    "    srcc = ss.spearmanr(gt, pred)[0]\n",
    "    print('MSE = {:.4f} LCC = {:.4f} SRCC = {:.4f}'.format(mse, lcc, srcc))\n",
    "    \n",
    "score(gt_mos, pred_mos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-dover",
   "metadata": {},
   "source": [
    "# Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load(path):\n",
    "    signal,sr = librosa.load(path, sr=16000)\n",
    "    #spec = np.abs(librosa.stft(signal, n_fft=512, hop_length=256, win_length=512, window=scipy.signal.hamming).T).astype(np.float32) # [time, 257]\n",
    "    #signal = signal[10000:60000]\n",
    "    return torch.as_tensor(signal).unsqueeze(0), torch.tensor([len(signal)], dtype=torch.long)#.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "confidential-paradise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveglow ...\n",
      "hifigan_v1 ...\n",
      "hifigan_v2 ...\n",
      "hifigan_v3 ...\n",
      "melgan ...\n",
      "val_dataset ...\n",
      "waveglow 3.3042843341827393\n",
      "hifigan_v1 3.5140740871429443\n",
      "hifigan_v2 3.4486920833587646\n",
      "hifigan_v3 3.422090530395508\n",
      "melgan 3.4947543144226074\n",
      "val_dataset 3.455531120300293\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_scores = {}\n",
    "\n",
    "for model_name in ['waveglow', 'hifigan_v1', 'hifigan_v2', 'hifigan_v3', 'melgan', 'val_dataset']:\n",
    "    print(model_name, '...')\n",
    "    model_scores[model_name] = []\n",
    "    for f in os.listdir('to_score/' + model_name):\n",
    "        signal, l = load(os.path.join('to_score', model_name, f))\n",
    "        with torch.no_grad():\n",
    "            res = model((signal.cuda(), l.cuda())).mean()\n",
    "            model_scores[model_name].append(res)\n",
    "            \n",
    "for model_name in model_scores:\n",
    "    print(model_name, torch.tensor(model_scores[model_name]).mean().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
